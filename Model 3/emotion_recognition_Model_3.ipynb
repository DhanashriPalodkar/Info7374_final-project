{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "emotion_recognition Model 3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "3Vr_qaNOSymq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "19e60da2-f481-4786-a66d-9220f4c216c0"
      },
      "cell_type": "code",
      "source": [
        "!pip install pydrive"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.6.7)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (1.11.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (4.0)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.11.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.5)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xCgZWfrOS7fY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "import tensorflow as tf\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U4X0mBskTAgO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "zip_file = drive.CreateFile({'id': '1C5FOGXLAAhvdaOuTWukGysyuj1NjIpVy'})\n",
        "zip_file.GetContentFile('complete_data_reduced.zip')\n",
        "validation_file = drive.CreateFile({'id':'1Xiy766wWCAo8gcNSyEFzwn2xaUHtxXqx'})\n",
        "validation_file.GetContentFile('validation_reduced.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bOex6BK6VlVm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.utils.np_utils import to_categorical  \n",
        "from keras.layers.core import Dense, Dropout, Flatten\n",
        "from keras.layers.convolutional import Convolution2D,SeparableConv2D,Conv2D\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.layers.pooling import MaxPooling2D,AveragePooling2D,GlobalAveragePooling2D\n",
        "from keras.layers import BatchNormalization,Activation,Input\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JntNFIicVrAZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import glob,string\n",
        "\n",
        "## file paths training data\n",
        "angry_path = 'combined_data_reduced/angry/*'\n",
        "closed_path = 'combined_data_reduced/closed/*'\n",
        "disgust_path = 'combined_data_reduced/disgusted/*'\n",
        "fear_path = 'combined_data_reduced/fearful/*'\n",
        "happy_path = 'combined_data_reduced/happy/*'\n",
        "neutral_path = 'combined_data_reduced/neutral/*'\n",
        "sad_path = 'combined_data_reduced/sad/*'\n",
        "surprised_path = 'combined_data_reduced/surprised/*'\n",
        "\n",
        "## file path validation\n",
        "angry_path_val = 'validation_reduced/angry/*'\n",
        "closed_path_val = 'validation_reduced/closed/*'\n",
        "disgust_path_val = 'validation_reduced/disgusted/*'\n",
        "fear_path_val = 'validation_reduced/fearful/*'\n",
        "happy_path_val = 'validation_reduced/happy/*'\n",
        "neutral_path_val = 'validation_reduced/neutral/*'\n",
        "sad_path_val = 'validation_reduced/sad/*'\n",
        "surprised_path_val = 'validation_reduced/surprised/*'\n",
        "\n",
        "\n",
        "#list files training\n",
        "angry_files=glob.glob(angry_path)\n",
        "closed_files = glob.glob(closed_path)\n",
        "disgust_files = glob.glob(disgust_path)\n",
        "fear_files = glob.glob(fear_path)\n",
        "happy_files = glob.glob(happy_path)\n",
        "neutral_files = glob.glob(neutral_path)\n",
        "sad_files = glob.glob(sad_path)\n",
        "surprised_path = glob.glob(surprised_path)\n",
        "\n",
        "#list files validation\n",
        "angry_files_val=glob.glob(angry_path_val)\n",
        "closed_files_val = glob.glob(closed_path_val)\n",
        "disgust_files_val = glob.glob(disgust_path_val)\n",
        "fear_files_val = glob.glob(fear_path_val)\n",
        "happy_files_val = glob.glob(happy_path_val)\n",
        "neutral_files_val = glob.glob(neutral_path_val)\n",
        "sad_files_val = glob.glob(sad_path_val)\n",
        "surprised_path_val = glob.glob(surprised_path_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PDkYWEeTVvBj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "num_epoch = 100\n",
        "\n",
        "training_directory='combined_data_reduced'\n",
        "validation_directory = 'validation_reduced'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ru_8yHgMV0TH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5682b3b6-de78-4892-994d-944ad835e643"
      },
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                        featurewise_center=False,\n",
        "                        featurewise_std_normalization=False,\n",
        "                        rotation_range=10,\n",
        "                        width_shift_range=0.1,\n",
        "                        height_shift_range=0.1,\n",
        "                        zoom_range=.1,\n",
        "                        horizontal_flip=True\n",
        "                                  )\n",
        "val_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                        featurewise_center=False,\n",
        "                        featurewise_std_normalization=False,\n",
        "                        rotation_range=10,\n",
        "                        width_shift_range=0.1,\n",
        "                        height_shift_range=0.1,\n",
        "                        zoom_range=.1,\n",
        "                        horizontal_flip=True\n",
        "                                )\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        training_directory,\n",
        "        target_size=(48,48),\n",
        "        batch_size=batch_size,\n",
        "        color_mode=\"grayscale\",\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        validation_directory,\n",
        "        target_size=(48,48),\n",
        "        batch_size=batch_size,\n",
        "        color_mode=\"grayscale\",\n",
        "        class_mode='categorical')\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 31527 images belonging to 8 classes.\n",
            "Found 7591 images belonging to 8 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZXcIFsEaV4kV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
        "    model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(128, kernel_size=(2, 2), activation='relu'))\n",
        "    model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "    model.add(Conv2D(256, kernel_size=(1, 1), activation='relu'))\n",
        "    model.add(Conv2D(512, kernel_size=(1, 1), activation='relu'))\n",
        "    model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(8, activation='softmax'))\n",
        " \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SSLRVTxxV9aq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "03819626-c888-44b3-f108-1fef974ea3bc"
      },
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "#model = create_model2()\n",
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 46, 46, 32)        320       \n",
            "_________________________________________________________________\n",
            "average_pooling2d_4 (Average (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 22, 22, 128)       16512     \n",
            "_________________________________________________________________\n",
            "average_pooling2d_5 (Average (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 11, 11, 256)       33024     \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 11, 11, 512)       131584    \n",
            "_________________________________________________________________\n",
            "average_pooling2d_6 (Average (None, 5, 5, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 5, 5, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 12800)             0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               3277056   \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 8)                 2056      \n",
            "=================================================================\n",
            "Total params: 3,460,552\n",
            "Trainable params: 3,460,552\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7DkEkeU0WCOY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "  json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x_H-XrZ9WGTM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b3409d25-68cb-494f-c018-50ba47b9ee7a"
      },
      "cell_type": "code",
      "source": [
        "!mkdir 'trained_models'"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘trained_models’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "us52K9wRWJSN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "patience =12\n",
        "\n",
        "#log_file_path = base_path + dataset_name + '_emotion_training.log'\n",
        "#csv_logger = CSVLogger(log_file_path, append=False)\n",
        "early_stop = EarlyStopping('val_loss', patience=patience)\n",
        "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1,\n",
        "                                  patience=int(patience/4), verbose=1)\n",
        "trained_models_path = 'trained_models3'\n",
        "model_names = trained_models_path + '.{epoch:02d}.hdf5'\n",
        "model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,\n",
        "                                                    save_best_only=True)\n",
        "callbacks = [model_checkpoint, early_stop, reduce_lr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DQNsaqJXWM-l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3723
        },
        "outputId": "ad68f81f-3583-4b1f-bdc0-fbb2ac593d95"
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer=Adam(lr=0.0001),metrics=['accuracy'])\n",
        "\n",
        "model_info = model.fit_generator(\n",
        "            train_generator,\n",
        "            \n",
        "            steps_per_epoch=950, #len(dataset)/batch_size\n",
        "            epochs=num_epoch,\n",
        "            callbacks=callbacks,\n",
        "            validation_data=validation_generator,\n",
        "            validation_steps=200,\n",
        "            workers=4,\n",
        "            shuffle=True)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "950/950 [==============================] - 79s 84ms/step - loss: 0.3528 - acc: 0.8750 - val_loss: 0.3492 - val_acc: 0.8750\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.34918, saving model to trained_models3.01.hdf5\n",
            "Epoch 2/100\n",
            "950/950 [==============================] - 80s 84ms/step - loss: 0.3452 - acc: 0.8751 - val_loss: 0.3424 - val_acc: 0.8749\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.34918 to 0.34243, saving model to trained_models3.02.hdf5\n",
            "Epoch 3/100\n",
            "950/950 [==============================] - 78s 83ms/step - loss: 0.3391 - acc: 0.8755 - val_loss: 0.3354 - val_acc: 0.8756\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.34243 to 0.33539, saving model to trained_models3.03.hdf5\n",
            "Epoch 4/100\n",
            "950/950 [==============================] - 79s 83ms/step - loss: 0.3345 - acc: 0.8761 - val_loss: 0.3334 - val_acc: 0.8760\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.33539 to 0.33338, saving model to trained_models3.04.hdf5\n",
            "Epoch 5/100\n",
            "950/950 [==============================] - 80s 85ms/step - loss: 0.3318 - acc: 0.8761 - val_loss: 0.3305 - val_acc: 0.8771\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.33338 to 0.33051, saving model to trained_models3.05.hdf5\n",
            "Epoch 6/100\n",
            "950/950 [==============================] - 82s 87ms/step - loss: 0.3287 - acc: 0.8771 - val_loss: 0.3247 - val_acc: 0.8777\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.33051 to 0.32473, saving model to trained_models3.06.hdf5\n",
            "Epoch 7/100\n",
            "950/950 [==============================] - 81s 85ms/step - loss: 0.3269 - acc: 0.8771 - val_loss: 0.3242 - val_acc: 0.8786\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.32473 to 0.32423, saving model to trained_models3.07.hdf5\n",
            "Epoch 8/100\n",
            "950/950 [==============================] - 81s 85ms/step - loss: 0.3243 - acc: 0.8780 - val_loss: 0.3209 - val_acc: 0.8784\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.32423 to 0.32091, saving model to trained_models3.08.hdf5\n",
            "Epoch 9/100\n",
            "950/950 [==============================] - 81s 86ms/step - loss: 0.3223 - acc: 0.8784 - val_loss: 0.3183 - val_acc: 0.8794\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.32091 to 0.31828, saving model to trained_models3.09.hdf5\n",
            "Epoch 10/100\n",
            "950/950 [==============================] - 81s 85ms/step - loss: 0.3196 - acc: 0.8791 - val_loss: 0.3168 - val_acc: 0.8803\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.31828 to 0.31682, saving model to trained_models3.10.hdf5\n",
            "Epoch 11/100\n",
            "950/950 [==============================] - 80s 85ms/step - loss: 0.3165 - acc: 0.8799 - val_loss: 0.3146 - val_acc: 0.8804\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.31682 to 0.31458, saving model to trained_models3.11.hdf5\n",
            "Epoch 12/100\n",
            "950/950 [==============================] - 80s 85ms/step - loss: 0.3163 - acc: 0.8797 - val_loss: 0.3121 - val_acc: 0.8808\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.31458 to 0.31210, saving model to trained_models3.12.hdf5\n",
            "Epoch 13/100\n",
            "950/950 [==============================] - 80s 84ms/step - loss: 0.3139 - acc: 0.8806 - val_loss: 0.3099 - val_acc: 0.8816\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.31210 to 0.30988, saving model to trained_models3.13.hdf5\n",
            "Epoch 14/100\n",
            "950/950 [==============================] - 81s 85ms/step - loss: 0.3118 - acc: 0.8816 - val_loss: 0.3090 - val_acc: 0.8823\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.30988 to 0.30897, saving model to trained_models3.14.hdf5\n",
            "Epoch 15/100\n",
            "950/950 [==============================] - 79s 83ms/step - loss: 0.3100 - acc: 0.8818 - val_loss: 0.3045 - val_acc: 0.8830\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.30897 to 0.30446, saving model to trained_models3.15.hdf5\n",
            "Epoch 16/100\n",
            "950/950 [==============================] - 80s 85ms/step - loss: 0.3082 - acc: 0.8820 - val_loss: 0.3047 - val_acc: 0.8835\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.30446\n",
            "Epoch 17/100\n",
            "950/950 [==============================] - 80s 84ms/step - loss: 0.3075 - acc: 0.8825 - val_loss: 0.3025 - val_acc: 0.8829\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.30446 to 0.30251, saving model to trained_models3.17.hdf5\n",
            "Epoch 18/100\n",
            "950/950 [==============================] - 81s 85ms/step - loss: 0.3049 - acc: 0.8829 - val_loss: 0.2984 - val_acc: 0.8851\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.30251 to 0.29838, saving model to trained_models3.18.hdf5\n",
            "Epoch 19/100\n",
            "950/950 [==============================] - 79s 83ms/step - loss: 0.3032 - acc: 0.8837 - val_loss: 0.3003 - val_acc: 0.8844\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.29838\n",
            "Epoch 20/100\n",
            "950/950 [==============================] - 80s 85ms/step - loss: 0.3020 - acc: 0.8841 - val_loss: 0.2976 - val_acc: 0.8852\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.29838 to 0.29762, saving model to trained_models3.20.hdf5\n",
            "Epoch 21/100\n",
            "950/950 [==============================] - 81s 85ms/step - loss: 0.3005 - acc: 0.8843 - val_loss: 0.2956 - val_acc: 0.8854\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.29762 to 0.29558, saving model to trained_models3.21.hdf5\n",
            "Epoch 22/100\n",
            "950/950 [==============================] - 79s 83ms/step - loss: 0.2998 - acc: 0.8847 - val_loss: 0.2936 - val_acc: 0.8867\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.29558 to 0.29355, saving model to trained_models3.22.hdf5\n",
            "Epoch 23/100\n",
            "950/950 [==============================] - 78s 82ms/step - loss: 0.2983 - acc: 0.8849 - val_loss: 0.2904 - val_acc: 0.8866\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.29355 to 0.29041, saving model to trained_models3.23.hdf5\n",
            "Epoch 24/100\n",
            "950/950 [==============================] - 79s 83ms/step - loss: 0.2964 - acc: 0.8857 - val_loss: 0.2925 - val_acc: 0.8862\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.29041\n",
            "Epoch 25/100\n",
            "950/950 [==============================] - 80s 84ms/step - loss: 0.2954 - acc: 0.8860 - val_loss: 0.2895 - val_acc: 0.8870\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.29041 to 0.28952, saving model to trained_models3.25.hdf5\n",
            "Epoch 26/100\n",
            "950/950 [==============================] - 78s 82ms/step - loss: 0.2941 - acc: 0.8861 - val_loss: 0.2868 - val_acc: 0.8888\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.28952 to 0.28679, saving model to trained_models3.26.hdf5\n",
            "Epoch 27/100\n",
            "950/950 [==============================] - 78s 82ms/step - loss: 0.2930 - acc: 0.8869 - val_loss: 0.2919 - val_acc: 0.8869\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.28679\n",
            "Epoch 28/100\n",
            "950/950 [==============================] - 79s 83ms/step - loss: 0.2923 - acc: 0.8867 - val_loss: 0.2871 - val_acc: 0.8885\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.28679\n",
            "Epoch 29/100\n",
            "950/950 [==============================] - 80s 84ms/step - loss: 0.2920 - acc: 0.8870 - val_loss: 0.2869 - val_acc: 0.8885\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.28679\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "Epoch 30/100\n",
            "950/950 [==============================] - 78s 82ms/step - loss: 0.2881 - acc: 0.8881 - val_loss: 0.2816 - val_acc: 0.8900\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.28679 to 0.28161, saving model to trained_models3.30.hdf5\n",
            "Epoch 31/100\n",
            "950/950 [==============================] - 78s 82ms/step - loss: 0.2876 - acc: 0.8885 - val_loss: 0.2827 - val_acc: 0.8896\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.28161\n",
            "Epoch 32/100\n",
            "950/950 [==============================] - 79s 83ms/step - loss: 0.2870 - acc: 0.8883 - val_loss: 0.2824 - val_acc: 0.8896\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.28161\n",
            "Epoch 33/100\n",
            "950/950 [==============================] - 79s 84ms/step - loss: 0.2864 - acc: 0.8889 - val_loss: 0.2820 - val_acc: 0.8904\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.28161\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "Epoch 34/100\n",
            "950/950 [==============================] - 78s 82ms/step - loss: 0.2869 - acc: 0.8884 - val_loss: 0.2803 - val_acc: 0.8907\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.28161 to 0.28033, saving model to trained_models3.34.hdf5\n",
            "Epoch 35/100\n",
            "950/950 [==============================] - 79s 83ms/step - loss: 0.2863 - acc: 0.8890 - val_loss: 0.2831 - val_acc: 0.8897\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.28033\n",
            "Epoch 36/100\n",
            "950/950 [==============================] - 78s 82ms/step - loss: 0.2855 - acc: 0.8889 - val_loss: 0.2815 - val_acc: 0.8899\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.28033\n",
            "Epoch 37/100\n",
            "950/950 [==============================] - 79s 83ms/step - loss: 0.2870 - acc: 0.8885 - val_loss: 0.2815 - val_acc: 0.8904\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.28033\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
            "Epoch 38/100\n",
            "950/950 [==============================] - 78s 82ms/step - loss: 0.2875 - acc: 0.8883 - val_loss: 0.2827 - val_acc: 0.8898\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.28033\n",
            "Epoch 39/100\n",
            "950/950 [==============================] - 79s 83ms/step - loss: 0.2857 - acc: 0.8886 - val_loss: 0.2794 - val_acc: 0.8906\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.28033 to 0.27941, saving model to trained_models3.39.hdf5\n",
            "Epoch 40/100\n",
            "950/950 [==============================] - 78s 82ms/step - loss: 0.2870 - acc: 0.8883 - val_loss: 0.2798 - val_acc: 0.8904\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.27941\n",
            "Epoch 41/100\n",
            "950/950 [==============================] - 79s 83ms/step - loss: 0.2856 - acc: 0.8890 - val_loss: 0.2832 - val_acc: 0.8898\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.27941\n",
            "Epoch 42/100\n",
            "950/950 [==============================] - 77s 82ms/step - loss: 0.2861 - acc: 0.8888 - val_loss: 0.2805 - val_acc: 0.8903\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.27941\n",
            "\n",
            "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
            "Epoch 43/100\n",
            "950/950 [==============================] - 78s 82ms/step - loss: 0.2877 - acc: 0.8881 - val_loss: 0.2800 - val_acc: 0.8912\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.27941\n",
            "Epoch 44/100\n",
            "950/950 [==============================] - 78s 82ms/step - loss: 0.2852 - acc: 0.8889 - val_loss: 0.2815 - val_acc: 0.8910\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.27941\n",
            "Epoch 45/100\n",
            "950/950 [==============================] - 79s 83ms/step - loss: 0.2854 - acc: 0.8885 - val_loss: 0.2819 - val_acc: 0.8900\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.27941\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
            "Epoch 46/100\n",
            "950/950 [==============================] - 78s 82ms/step - loss: 0.2851 - acc: 0.8890 - val_loss: 0.2809 - val_acc: 0.8908\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.27941\n",
            "Epoch 47/100\n",
            "950/950 [==============================] - 79s 83ms/step - loss: 0.2869 - acc: 0.8888 - val_loss: 0.2804 - val_acc: 0.8907\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.27941\n",
            "Epoch 48/100\n",
            "950/950 [==============================] - 78s 82ms/step - loss: 0.2868 - acc: 0.8885 - val_loss: 0.2823 - val_acc: 0.8898\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.27941\n",
            "\n",
            "Epoch 00048: ReduceLROnPlateau reducing learning rate to 9.999999717180686e-11.\n",
            "Epoch 49/100\n",
            "950/950 [==============================] - 80s 84ms/step - loss: 0.2866 - acc: 0.8886 - val_loss: 0.2814 - val_acc: 0.8906\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.27941\n",
            "Epoch 50/100\n",
            "950/950 [==============================] - 79s 83ms/step - loss: 0.2856 - acc: 0.8892 - val_loss: 0.2824 - val_acc: 0.8897\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.27941\n",
            "Epoch 51/100\n",
            "950/950 [==============================] - 80s 84ms/step - loss: 0.2858 - acc: 0.8892 - val_loss: 0.2800 - val_acc: 0.8907\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.27941\n",
            "\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 9.99999943962493e-12.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}